{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Ridge1_ridge-lb-0-41943.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtS9H_6PzLQe"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "  \r\n",
        "- 해당 커널은 brand name, item subcategory 컬럼에 있었던 일관성 없었던 텍스트 데이터들에 대하여  Symspell, Damerau-Levenshtein distance, 정규패턴식을 이용하여 feature들을 통일시켜주는 수준 높은 기법을 이용하였다. 이는 또한 고급 결측치 대입 방법이 되기도 하였다.\r\n",
        "  \r\n",
        "- FeatureUnion, Pipeline을 잘 사용했으며, 특히 sklearn.base의 BaseEstimator과 TransformerMixin을 상속하여 Customized한 ColumnTransformer을 사용한 것은 신선하게 느껴졌다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IeOndx9z4c9"
      },
      "source": [
        "import multiprocessing as mp\r\n",
        "import pandas as pd\r\n",
        "from time import time\r\n",
        "from scipy.sparse import csr_matrix\r\n",
        "import os\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\r\n",
        "from sklearn.metrics import mean_squared_log_error\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "import numpy as np\r\n",
        "import gc\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "import re\r\n",
        "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\r\n",
        "\r\n",
        "os.environ['MKL_NUM_THREADS'] = '4'\r\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\r\n",
        "os.environ['JOBLIB_START_METHOD'] = 'forkserver'\r\n",
        "\r\n",
        "INPUT_PATH = r'../input'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "75353587-e3ba-4fdc-8377-2f1f94334d97",
        "_cell_guid": "9ff7958f-9ef4-46d4-9dce-e2201a69c77c",
        "trusted": true,
        "id": "RsNrJPtdJYW8"
      },
      "source": [
        "# 형태적 유사성을 정의하는 방법 (string distance) 중 하나인 Damerau-Levenshtein distance\n",
        "\n",
        "def dameraulevenshtein(seq1, seq2):\n",
        "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
        "\n",
        "    This method has not been modified from the original.\n",
        "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
        "\n",
        "    This distance is the number of additions, deletions, substitutions,\n",
        "    and transpositions needed to transform the first sequence into the\n",
        "    second. Although generally used with strings, any sequences of\n",
        "    comparable objects will work.\n",
        "\n",
        "    Transpositions are exchanges of *consecutive* characters; all other\n",
        "    operations are self-explanatory.\n",
        "\n",
        "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
        "    lengths of the two sequences.\n",
        "\n",
        "    >>> dameraulevenshtein('ba', 'abc')\n",
        "    2\n",
        "    >>> dameraulevenshtein('fee', 'deed')\n",
        "    2\n",
        "\n",
        "    It works with arbitrary sequences too:\n",
        "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
        "    2\n",
        "    \"\"\"\n",
        "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
        "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
        "    # However, only the current and two previous rows are needed at once,\n",
        "    # so we only store those.\n",
        "    oneago = None\n",
        "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
        "    for x in range(len(seq1)):\n",
        "        # Python lists wrap around for negative indices, so put the\n",
        "        # leftmost column at the *end* of the list. This matches with\n",
        "        # the zero-indexed strings and saves extra calculation.\n",
        "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
        "        for y in range(len(seq2)):\n",
        "            delcost = oneago[y] + 1\n",
        "            addcost = thisrow[y - 1] + 1\n",
        "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
        "            thisrow[y] = min(delcost, addcost, subcost)\n",
        "            # This block deals with transpositions\n",
        "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
        "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
        "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
        "    return thisrow[len(seq2) - 1]\n",
        "\n",
        "\n",
        "class SymSpell:\n",
        "    def __init__(self, max_edit_distance=3, verbose=0):\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        # 0: top suggestion\n",
        "        # 1: all suggestions of smallest edit distance\n",
        "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
        "\n",
        "        self.dictionary = {}\n",
        "        self.longest_word_length = 0\n",
        "\n",
        "    def get_deletes_list(self, w):\n",
        "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
        "           deleted\"\"\"\n",
        "\n",
        "        deletes = []\n",
        "        queue = [w]\n",
        "        for d in range(self.max_edit_distance):\n",
        "            temp_queue = []\n",
        "            for word in queue:\n",
        "                if len(word) > 1:\n",
        "                    for c in range(len(word)):  # character index\n",
        "                        word_minus_c = word[:c] + word[c + 1:]\n",
        "                        if word_minus_c not in deletes:\n",
        "                            deletes.append(word_minus_c)\n",
        "                        if word_minus_c not in temp_queue:\n",
        "                            temp_queue.append(word_minus_c)\n",
        "            queue = temp_queue\n",
        "\n",
        "        return deletes\n",
        "\n",
        "    def create_dictionary_entry(self, w):\n",
        "        '''add word and its derived deletions to dictionary'''\n",
        "        # check if word is already in dictionary\n",
        "        # dictionary entries are in the form: (list of suggested corrections,\n",
        "        # frequency of word in corpus)\n",
        "        new_real_word_added = False\n",
        "        if w in self.dictionary:\n",
        "            # increment count of word in corpus\n",
        "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
        "        else:\n",
        "            self.dictionary[w] = ([], 1)\n",
        "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
        "\n",
        "        if self.dictionary[w][1] == 1:\n",
        "            # first appearance of word in corpus\n",
        "            # n.b. word may already be in dictionary as a derived word\n",
        "            # (deleting character from a real word)\n",
        "            # but counter of frequency of word in corpus is not incremented\n",
        "            # in those cases)\n",
        "            new_real_word_added = True\n",
        "            deletes = self.get_deletes_list(w)\n",
        "            for item in deletes:\n",
        "                if item in self.dictionary:\n",
        "                    # add (correct) word to delete's suggested correction list\n",
        "                    self.dictionary[item][0].append(w)\n",
        "                else:\n",
        "                    # note frequency of word in corpus is not incremented\n",
        "                    self.dictionary[item] = ([w], 0)\n",
        "\n",
        "        return new_real_word_added\n",
        "\n",
        "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        for line in arr:\n",
        "            # separate by words by non-alphabetical characters\n",
        "            words = re.findall(token_pattern, line.lower())\n",
        "            for word in words:\n",
        "                total_word_count += 1\n",
        "                if self.create_dictionary_entry(word):\n",
        "                    unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def create_dictionary(self, fname):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        with open(fname) as file:\n",
        "            for line in file:\n",
        "                # separate by words by non-alphabetical characters\n",
        "                words = re.findall('[a-z]+', line.lower())\n",
        "                for word in words:\n",
        "                    total_word_count += 1\n",
        "                    if self.create_dictionary_entry(word):\n",
        "                        unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def get_suggestions(self, string, silent=False):\n",
        "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
        "           spelled word\"\"\"\n",
        "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
        "            if not silent:\n",
        "                print(\"no items in dictionary within maximum edit distance\")\n",
        "            return []\n",
        "\n",
        "        suggest_dict = {}\n",
        "        min_suggest_len = float('inf')\n",
        "\n",
        "        queue = [string]\n",
        "        q_dictionary = {}  # items other than string that we've checked\n",
        "\n",
        "        while len(queue) > 0:\n",
        "            q_item = queue[0]  # pop\n",
        "            queue = queue[1:]\n",
        "\n",
        "            # early exit\n",
        "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
        "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
        "                break\n",
        "\n",
        "            # process queue item\n",
        "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
        "                if self.dictionary[q_item][1] > 0:\n",
        "                    # word is in dictionary, and is a word from the corpus, and\n",
        "                    # not already in suggestion list so add to suggestion\n",
        "                    # dictionary, indexed by the word with value (frequency in\n",
        "                    # corpus, edit distance)\n",
        "                    # note q_items that are not the input string are shorter\n",
        "                    # than input string since only deletes are added (unless\n",
        "                    # manual dictionary corrections are added)\n",
        "                    assert len(string) >= len(q_item)\n",
        "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
        "                                            len(string) - len(q_item))\n",
        "                    # early exit\n",
        "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
        "                        break\n",
        "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
        "                        min_suggest_len = len(string) - len(q_item)\n",
        "\n",
        "                # the suggested corrections for q_item as stored in\n",
        "                # dictionary (whether or not q_item itself is a valid word\n",
        "                # or merely a delete) can be valid corrections\n",
        "                for sc_item in self.dictionary[q_item][0]:\n",
        "                    if sc_item not in suggest_dict:\n",
        "\n",
        "                        # compute edit distance\n",
        "                        # suggested items should always be longer\n",
        "                        # (unless manual corrections are added)\n",
        "                        assert len(sc_item) > len(q_item)\n",
        "\n",
        "                        # q_items that are not input should be shorter\n",
        "                        # than original string\n",
        "                        # (unless manual corrections added)\n",
        "                        assert len(q_item) <= len(string)\n",
        "\n",
        "                        if len(q_item) == len(string):\n",
        "                            assert q_item == string\n",
        "                            item_dist = len(sc_item) - len(q_item)\n",
        "\n",
        "                        # item in suggestions list should not be the same as\n",
        "                        # the string itself\n",
        "                        assert sc_item != string\n",
        "\n",
        "                        # calculate edit distance using, for example,\n",
        "                        # Damerau-Levenshtein distance\n",
        "                        item_dist = dameraulevenshtein(sc_item, string)\n",
        "\n",
        "                        # do not add words with greater edit distance if\n",
        "                        # verbose setting not on\n",
        "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
        "                            pass\n",
        "                        elif item_dist <= self.max_edit_distance:\n",
        "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
        "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
        "                            if item_dist < min_suggest_len:\n",
        "                                min_suggest_len = item_dist\n",
        "\n",
        "                        # depending on order words are processed, some words\n",
        "                        # with different edit distances may be entered into\n",
        "                        # suggestions; trim suggestion dictionary if verbose\n",
        "                        # setting not on\n",
        "                        if self.verbose < 2:\n",
        "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
        "\n",
        "            # now generate deletes (e.g. a substring of string or of a delete)\n",
        "            # from the queue item\n",
        "            # as additional items to check -- add to end of queue\n",
        "            assert len(string) >= len(q_item)\n",
        "\n",
        "            # do not add words with greater edit distance if verbose setting\n",
        "            # is not on\n",
        "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
        "                pass\n",
        "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
        "                for c in range(len(q_item)):  # character index\n",
        "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
        "                    if word_minus_c not in q_dictionary:\n",
        "                        queue.append(word_minus_c)\n",
        "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
        "\n",
        "        # queue is now empty: convert suggestions in dictionary to\n",
        "        # list for output\n",
        "        if not silent and self.verbose != 0:\n",
        "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
        "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "\n",
        "        # output option 1\n",
        "        # sort results by ascending order of edit distance and descending\n",
        "        # order of frequency\n",
        "        #     and return list of suggested word corrections only:\n",
        "        # return sorted(suggest_dict, key = lambda x:\n",
        "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
        "\n",
        "        # output option 2\n",
        "        # return list of suggestions with (correction,\n",
        "        #                                  (frequency in corpus, edit distance)):\n",
        "        as_list = suggest_dict.items()\n",
        "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
        "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
        "\n",
        "        if self.verbose == 0:\n",
        "            return outlist[0]\n",
        "        else:\n",
        "            return outlist\n",
        "\n",
        "        '''\n",
        "        Option 1:\n",
        "        ['file', 'five', 'fire', 'fine', ...]\n",
        "\n",
        "        Option 2:\n",
        "        [('file', (5, 0)),\n",
        "         ('five', (67, 1)),\n",
        "         ('fire', (54, 1)),\n",
        "         ('fine', (17, 1))...]  \n",
        "        '''\n",
        "\n",
        "    def best_word(self, s, silent=False):\n",
        "        try:\n",
        "            return self.get_suggestions(s, silent)[0]\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfNF2B3hz1eU"
      },
      "source": [
        "# label_encoding과 같은 작업후 이전 column들을 drop 하는 customized class 정의 (BaseEstimator, TransformerMixin 상속)\r\n",
        "class ItemSelector(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, field, start_time=time()):\r\n",
        "        self.field = field\r\n",
        "        self.start_time = start_time\r\n",
        "\r\n",
        "    def fit(self, x, y=None):\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, dataframe):\r\n",
        "        print(f'[{time()-self.start_time}] select {self.field}')\r\n",
        "        dt = dataframe[self.field].dtype\r\n",
        "        if is_categorical_dtype(dt):\r\n",
        "            return dataframe[self.field].cat.codes[:, None]\r\n",
        "        elif is_numeric_dtype(dt):\r\n",
        "            return dataframe[self.field][:, None]\r\n",
        "        else:\r\n",
        "            return dataframe[self.field]\r\n",
        "\r\n",
        "\r\n",
        "class DropColumnsByDf(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, min_df=1, max_df=1.0):\r\n",
        "        self.min_df = min_df\r\n",
        "        self.max_df = max_df\r\n",
        "\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        m = X.tocsc() # compressed sparse column 왜 사용?\r\n",
        "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1 # output\r\n",
        "        if self.max_df < 1.0:\r\n",
        "            max_df = m.shape[0] * self.max_df\r\n",
        "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, X, y=None):\r\n",
        "        m = X.tocsc()\r\n",
        "        return m[:, self.nnz_cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whBDdYZD00Wf"
      },
      "source": [
        "def get_rmsle(y_true, y_pred):\r\n",
        "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\r\n",
        "\r\n",
        "\r\n",
        "def split_cat(text):\r\n",
        "    try:\r\n",
        "        cats = text.split(\"/\")\r\n",
        "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\r\n",
        "    except:\r\n",
        "        print(\"no category\")\r\n",
        "        return 'other', 'other', 'other', 'other/other'\r\n",
        "\r\n",
        "\r\n",
        "# oneword, manywords의 사전을 따로 만들고 따로 통일시킴\r\n",
        "def brands_filling(dataset):\r\n",
        "    vc = dataset['brand_name'].value_counts()\r\n",
        "    brands = vc[vc > 0].index\r\n",
        "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\r\n",
        "\r\n",
        "    many_w_brands = brands[brands.str.contains(' ')]\r\n",
        "    one_w_brands = brands[~brands.str.contains(' ')]\r\n",
        "\r\n",
        "    ss2 = SymSpell(max_edit_distance=0)\r\n",
        "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\r\n",
        "\r\n",
        "    ss1 = SymSpell(max_edit_distance=0)\r\n",
        "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\r\n",
        "\r\n",
        "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\r\n",
        "\r\n",
        "    def find_in_str_ss2(row):\r\n",
        "        for doc_word in two_words_re.finditer(row):\r\n",
        "            print(doc_word)\r\n",
        "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\r\n",
        "            if suggestion is not None:\r\n",
        "                return doc_word.group(1)\r\n",
        "        return ''\r\n",
        "\r\n",
        "    def find_in_list_ss1(list):\r\n",
        "        for doc_word in list:\r\n",
        "            suggestion = ss1.best_word(doc_word, silent=True)\r\n",
        "            if suggestion is not None:\r\n",
        "                return doc_word\r\n",
        "        return ''\r\n",
        "\r\n",
        "    def find_in_list_ss2(list):\r\n",
        "        for doc_word in list:\r\n",
        "            suggestion = ss2.best_word(doc_word, silent=True)\r\n",
        "            if suggestion is not None:\r\n",
        "                return doc_word\r\n",
        "        return ''\r\n",
        "\r\n",
        "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\r\n",
        "\r\n",
        "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\r\n",
        "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\r\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\r\n",
        "\r\n",
        "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\r\n",
        "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\r\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\r\n",
        "\r\n",
        "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\r\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\r\n",
        "\r\n",
        "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\r\n",
        "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\r\n",
        "\r\n",
        "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\r\n",
        "\r\n",
        "    del ss1, ss2\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_regex(dataset, start_time=time()):\r\n",
        "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\r\n",
        "    karats_repl = r'\\1k\\4'\r\n",
        "\r\n",
        "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\r\n",
        "    unit_repl = r'\\1\\2\\3'\r\n",
        "\r\n",
        "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\r\n",
        "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\r\n",
        "    print(f'[{time() - start_time}] Karats normalized.')\r\n",
        "\r\n",
        "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\r\n",
        "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\r\n",
        "    print(f'[{time() - start_time}] Units glued.')\r\n",
        "\r\n",
        "# log1p(y), name = name+brandname, item_desc = name+brandname + sub_cat1 + sub_cat2 + gen_cat\r\n",
        "def preprocess_pandas(train, test, start_time=time()):\r\n",
        "    train = train[train.price > 0.0].reset_index(drop=True)\r\n",
        "    print('Train shape without zero price: ', train.shape)\r\n",
        "\r\n",
        "    nrow_train = train.shape[0]\r\n",
        "    y_train = np.log1p(train[\"price\"])\r\n",
        "    merge: pd.DataFrame = pd.concat([train, test])\r\n",
        "\r\n",
        "    del train\r\n",
        "    del test\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\r\n",
        "    print(f'[{time() - start_time}] Has_category filled.')\r\n",
        "\r\n",
        "    merge['category_name'] = merge['category_name'] \\\r\n",
        "        .fillna('other/other/other') \\\r\n",
        "        .str.lower() \\\r\n",
        "        .astype(str)\r\n",
        "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = \\\r\n",
        "        zip(*merge['category_name'].apply(lambda x: split_cat(x)))\r\n",
        "    print(f'[{time() - start_time}] Split categories completed.')\r\n",
        "\r\n",
        "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\r\n",
        "    print(f'[{time() - start_time}] Has_brand filled.')\r\n",
        "\r\n",
        "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\r\n",
        "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\r\n",
        "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\r\n",
        "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\r\n",
        "\r\n",
        "    merge['name'] = merge['name'] \\\r\n",
        "        .fillna('') \\\r\n",
        "        .str.lower() \\\r\n",
        "        .astype(str)\r\n",
        "    merge['brand_name'] = merge['brand_name'] \\\r\n",
        "        .fillna('') \\\r\n",
        "        .str.lower() \\\r\n",
        "        .astype(str)\r\n",
        "    merge['item_description'] = merge['item_description'] \\\r\n",
        "        .fillna('') \\\r\n",
        "        .str.lower() \\\r\n",
        "        .replace(to_replace='No description yet', value='')\r\n",
        "    print(f'[{time() - start_time}] Missing filled.')\r\n",
        "\r\n",
        "    preprocess_regex(merge, start_time)\r\n",
        "\r\n",
        "    brands_filling(merge)\r\n",
        "    print(f'[{time() - start_time}] Brand name filled.')\r\n",
        "\r\n",
        "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\r\n",
        "    print(f'[{time() - start_time}] Name concancenated.')\r\n",
        "\r\n",
        "    merge['item_description'] = merge['item_description'] \\\r\n",
        "                                + ' ' + merge['name'] \\\r\n",
        "                                + ' ' + merge['subcat_1'] \\\r\n",
        "                                + ' ' + merge['subcat_2'] \\\r\n",
        "                                + ' ' + merge['general_cat'] \\\r\n",
        "                                + ' ' + merge['brand_name']\r\n",
        "    print(f'[{time() - start_time}] Item description concatenated.')\r\n",
        "\r\n",
        "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\r\n",
        "\r\n",
        "    return merge, y_train, nrow_train\r\n",
        "\r\n",
        "\r\n",
        "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\r\n",
        "    t = train.tocsc()\r\n",
        "    v = valid.tocsc()\r\n",
        "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\r\n",
        "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\r\n",
        "    nnz_cols = nnz_train & nnz_valid\r\n",
        "    res = t[:, nnz_cols], v[:, nnz_cols]\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  #  mp.set_start_method('forkserver', True)\r\n",
        "\r\n",
        "start_time = time()\r\n",
        "\r\n",
        "train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\r\n",
        "                      engine='c',\r\n",
        "                      dtype={'item_condition_id': 'category',\r\n",
        "                              'shipping': 'category'}\r\n",
        "                      )\r\n",
        "test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\r\n",
        "                      engine='c',\r\n",
        "                      dtype={'item_condition_id': 'category',\r\n",
        "                            'shipping': 'category'}\r\n",
        "                      )\r\n",
        "print(f'[{time() - start_time}] Finished to load data')\r\n",
        "print('Train shape: ', train.shape)\r\n",
        "print('Test shape: ', test.shape)\r\n",
        "\r\n",
        "submission: pd.DataFrame = test[['test_id']]\r\n",
        "\r\n",
        "merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\r\n",
        "\r\n",
        "meta_params = {'name_ngram': (1, 2),\r\n",
        "                'name_max_f': 75000,\r\n",
        "                'name_min_df': 10,\r\n",
        "\r\n",
        "                'category_ngram': (2, 3),\r\n",
        "                'category_token': '.+',\r\n",
        "                'category_min_df': 10,\r\n",
        "\r\n",
        "                'brand_min_df': 10,\r\n",
        "\r\n",
        "                'desc_ngram': (1, 3),\r\n",
        "                'desc_max_f': 150000,\r\n",
        "                'desc_max_df': 0.5,\r\n",
        "                'desc_min_df': 10}\r\n",
        "\r\n",
        "stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this', ])\r\n",
        "# 'i', 'so', 'its', 'am', 'are'])\r\n",
        "\r\n",
        "vectorizer = FeatureUnion([\r\n",
        "    ('name', Pipeline([\r\n",
        "        ('select', ItemSelector('name', start_time=start_time)),\r\n",
        "        ('transform', HashingVectorizer(\r\n",
        "            ngram_range=(1, 2),\r\n",
        "            n_features=2 ** 27,\r\n",
        "            norm='l2',\r\n",
        "            lowercase=False,\r\n",
        "            stop_words=stopwords\r\n",
        "        )),\r\n",
        "        ('drop_cols', DropColumnsByDf(min_df=2))\r\n",
        "    ])),\r\n",
        "    ('category_name', Pipeline([\r\n",
        "        ('select', ItemSelector('category_name', start_time=start_time)),\r\n",
        "        ('transform', HashingVectorizer(\r\n",
        "            ngram_range=(1, 1),\r\n",
        "            token_pattern='.+',\r\n",
        "            tokenizer=split_cat,\r\n",
        "            n_features=2 ** 27,\r\n",
        "            norm='l2',\r\n",
        "            lowercase=False\r\n",
        "        )),\r\n",
        "        ('drop_cols', DropColumnsByDf(min_df=2))\r\n",
        "    ])),\r\n",
        "    ('brand_name', Pipeline([\r\n",
        "        ('select', ItemSelector('brand_name', start_time=start_time)),\r\n",
        "        ('transform', CountVectorizer(\r\n",
        "            token_pattern='.+',\r\n",
        "            min_df=2,\r\n",
        "            lowercase=False\r\n",
        "        )),\r\n",
        "    ])),\r\n",
        "    ('gencat_cond', Pipeline([\r\n",
        "        ('select', ItemSelector('gencat_cond', start_time=start_time)),\r\n",
        "        ('transform', CountVectorizer(\r\n",
        "            token_pattern='.+',\r\n",
        "            min_df=2,\r\n",
        "            lowercase=False\r\n",
        "        )),\r\n",
        "    ])),\r\n",
        "    ('subcat_1_cond', Pipeline([\r\n",
        "        ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\r\n",
        "        ('transform', CountVectorizer(\r\n",
        "            token_pattern='.+',\r\n",
        "            min_df=2,\r\n",
        "            lowercase=False\r\n",
        "        )),\r\n",
        "    ])),\r\n",
        "    ('subcat_2_cond', Pipeline([\r\n",
        "        ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\r\n",
        "        ('transform', CountVectorizer(\r\n",
        "            token_pattern='.+',\r\n",
        "            min_df=2,\r\n",
        "            lowercase=False\r\n",
        "        )),\r\n",
        "    ])),\r\n",
        "    ('has_brand', Pipeline([\r\n",
        "        ('select', ItemSelector('has_brand', start_time=start_time)),\r\n",
        "        ('ohe', OneHotEncoder())\r\n",
        "    ])),\r\n",
        "    ('shipping', Pipeline([\r\n",
        "        ('select', ItemSelector('shipping', start_time=start_time)),\r\n",
        "        ('ohe', OneHotEncoder())\r\n",
        "    ])),\r\n",
        "    ('item_condition_id', Pipeline([\r\n",
        "        ('select', ItemSelector('item_condition_id', start_time=start_time)),\r\n",
        "        ('ohe', OneHotEncoder())\r\n",
        "    ])),\r\n",
        "    ('item_description', Pipeline([\r\n",
        "        ('select', ItemSelector('item_description', start_time=start_time)),\r\n",
        "        ('hash', HashingVectorizer(\r\n",
        "            ngram_range=(1, 3),\r\n",
        "            n_features=2 ** 27,\r\n",
        "            dtype=np.float32,\r\n",
        "            norm='l2',\r\n",
        "            lowercase=False,\r\n",
        "            stop_words=stopwords\r\n",
        "        )),\r\n",
        "        ('drop_cols', DropColumnsByDf(min_df=2)),\r\n",
        "    ]))\r\n",
        "], n_jobs=1)\r\n",
        "\r\n",
        "sparse_merge = vectorizer.fit_transform(merge)\r\n",
        "print(f'[{time() - start_time}] Merge vectorized')\r\n",
        "print(sparse_merge.shape)\r\n",
        "\r\n",
        "tfidf_transformer = TfidfTransformer()\r\n",
        "\r\n",
        "X = tfidf_transformer.fit_transform(sparse_merge)\r\n",
        "print(f'[{time() - start_time}] TF/IDF completed')\r\n",
        "\r\n",
        "X_train = X[:nrow_train]\r\n",
        "print(X_train.shape)\r\n",
        "\r\n",
        "X_test = X[nrow_train:]\r\n",
        "del merge\r\n",
        "del sparse_merge\r\n",
        "del vectorizer\r\n",
        "del tfidf_transformer\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\r\n",
        "print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\r\n",
        "ridge.fit(X_train, y_train)\r\n",
        "print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\r\n",
        "\r\n",
        "predsR = ridge.predict(X_test)\r\n",
        "print(f'[{time() - start_time}] Predict Ridge completed.')\r\n",
        "\r\n",
        "submission.loc[:, 'price'] = np.expm1(predsR)\r\n",
        "submission.loc[submission['price'] < 0.0, 'price'] = 0.0\r\n",
        "submission.to_csv(\"submission_ridge.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}