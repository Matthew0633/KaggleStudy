{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "D7_CNN6_blend-of-lstm-and-cnn-with-4-embeddings-1200d.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkeLvNk4mGd1"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "- glove, fasttext, word2vec, paragram embedding concat\r\n",
        "- LSTM : 각 embedding matrix를 embedding해서 최종 embedding matrix -> LSTM\r\n",
        "- CNN : (Conv, maxpooling)을 병렬로 여러개 두고 output concat\r\n",
        "- F1 score max하는 target 1에 대한 threshold greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "cc406dac1c0854793fe8672741f97f773f829909",
        "id": "KPxHxrFp2gJk"
      },
      "source": [
        "* Based on SRK's kernel: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
        "* Concatenate embeddings instead of blending predictions. Added SpatialDropout1D.\n",
        "* Added word2vec from https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go.\n",
        "* Combined Vladimir Demidov's 2DCNN textClassifier: https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "* Modified the code to choose best threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "144fe5891359c058bba89c6bdef4054d3f324f34",
        "id": "ullY-oCq2gJl"
      },
      "source": [
        "## Settings:\n",
        "    \n",
        "# some config values \n",
        "embed_size = 300 # how big is each word vector\n",
        "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "maxlen = 100 # max number of words in a question to use\n",
        "\n",
        "S_DROPOUT = 0.4\n",
        "DROPOUT = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "Yy7ufQd-2gJm"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import gc\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "VsFzD3JR2gJn"
      },
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "print(\"Train shape : \",train_df.shape)\n",
        "print(\"Test shape : \",test_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ba5a1b8109dee2c9fbc628d5da4a7c3447d42fb8",
        "id": "esL0UajO2gJn"
      },
      "source": [
        "## split to train and val\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
        "\n",
        "## fill up the missing values\n",
        "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
        "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
        "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "val_X = tokenizer.texts_to_sequences(val_X)\n",
        "test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "## Pad the sentences \n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "## Get the target values\n",
        "train_y = train_df['target'].values\n",
        "val_y = val_df['target'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "717f7dcd5ccf71e83d0f062e221c46db39845e4d",
        "id": "4KtO3XML2gJo"
      },
      "source": [
        "**Glove Embeddings:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "23f130e80159bb1701e449e2e91199dbfff1f1d4",
        "id": "D9J6k0EI2gJo"
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE)) # {word : array}\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n",
        "\n",
        "del embeddings_index; gc.collect() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "bc6bab22dd12a09378f4b8b159cb7a5d88a3e7c0",
        "id": "c91vliGE2gJo"
      },
      "source": [
        "**Wiki News FastText Embeddings:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "6f3d0fd28dd2b04eaccb732b96b872e5a223d962",
        "id": "Lq_vPNw12gJo"
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_2[i] = embedding_vector\n",
        "        \n",
        "del embeddings_index; gc.collect()         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4ca44ac68bf404b9c26e07fbcc9c8ac793e04510",
        "id": "KKG4y2eE2gJp"
      },
      "source": [
        "**Paragram Embeddings:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "25ec1aac4aedbf431a2d30de64030ce8e3203c18",
        "id": "-zsufolY2gJp"
      },
      "source": [
        "EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n",
        "\n",
        "del embeddings_index; gc.collect()         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4474db5a07969ca0703c65c3f95c2b0f61c1468b",
        "id": "hOPt36rG2gJp"
      },
      "source": [
        "**Word2vec Embeddings:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9b83944a5ad66f4b5d60d57bed12b9c19035a19a",
        "id": "M8MRwaBY2gJq"
      },
      "source": [
        "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
        "embeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix_4 = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    if word in embeddings_index:\n",
        "        embedding_vector = embeddings_index.get_vector(word)\n",
        "        embedding_matrix_4[i] = embedding_vector\n",
        "        \n",
        "del embeddings_index; gc.collect()         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "94e1be128208411c4d95a26610a4d58e66be013c",
        "id": "pr4v25hi2gJq"
      },
      "source": [
        "** Combine :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "957c8187e76769e3165e900a9784fc4bb30ffca4",
        "id": "0uf0LVHg2gJr"
      },
      "source": [
        "embedding_matrix = np.concatenate((embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4), axis=1) # (nb_words, emb_dim *4)\n",
        "del embedding_matrix_1, embedding_matrix_2, embedding_matrix_3, embedding_matrix_4\n",
        "gc.collect()\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "77c68caa1c640b9e463a196cbca50304ae134d75",
        "id": "04JRa-ec2gJr"
      },
      "source": [
        "**LSTM:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ef7e53b15cf73d04747f1d225f25a3c37c1eef5f",
        "id": "HDSAvfw52gJr"
      },
      "source": [
        "# https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings\n",
        "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
        "\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size * 4, weights=[embedding_matrix])(inp)\n",
        "x = SpatialDropout1D(S_DROPOUT)(x)\n",
        "x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "conc = concatenate([avg_pool, max_pool])\n",
        "x = Dense(16, activation=\"relu\")(conc)\n",
        "x = Dropout(DROPOUT)(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cc188f2787ea7b98d3a40953a95a5fc09ff2764d",
        "id": "D4Z8U-kW2gJs"
      },
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3432b48b6995e297fe33748c76174bd0e56c74a2",
        "id": "B5mOd__X2gJs"
      },
      "source": [
        "pred_val_lstm_y = model.predict([val_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "99cb9f6145da909bd7436e46d47547efc097499d",
        "id": "ulB43OXZ2gJt"
      },
      "source": [
        "pred_test_lstm_y = model.predict([test_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "af087d21bdb4358701e31aded6b522accd5a8a64",
        "id": "evUwPhRQ2gJt"
      },
      "source": [
        "del all_embs, model, inp, x\n",
        "import gc; gc.collect()\n",
        "time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "230dfd912022ba8a270766d3d5805d170afcfca6",
        "id": "RYszPk9i2gJt"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ddbc4f12d991399fa735066e2f0656b94b551602",
        "id": "FYHFZZ5R2gJt"
      },
      "source": [
        "# https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "\n",
        "filter_sizes = [1,2,3,5]\n",
        "num_filters = 36\n",
        "\n",
        "inp = Input(shape=(maxlen,))\n",
        "x = Embedding(max_features, embed_size * 4, weights=[embedding_matrix])(inp)\n",
        "x = SpatialDropout1D(S_DROPOUT)(x)\n",
        "x = Reshape((maxlen, embed_size * 4, 1))(x)\n",
        "\n",
        "maxpool_pool = []\n",
        "for i in range(len(filter_sizes)):\n",
        "    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size * 4),\n",
        "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
        "    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
        "\n",
        "z = Concatenate(axis=1)(maxpool_pool)   \n",
        "z = Flatten()(z)\n",
        "z = Dropout(DROPOUT)(z)\n",
        "\n",
        "outp = Dense(1, activation=\"sigmoid\")(z)\n",
        "\n",
        "model = Model(inputs=inp, outputs=outp)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e5b2b647d5f65123db9889afe53fd0e5a86b2a9e",
        "id": "BDjbkzJ22gJu"
      },
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a5f010df43d7b6aeb7c3d040d1dd4ce839b6b4ca",
        "id": "C4smMfh22gJu"
      },
      "source": [
        "pred_val_cnn_y = model.predict([val_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "688b1400e59b9ae1a321efd6d16e9065b353f49f",
        "id": "-fKakph92gJu"
      },
      "source": [
        "pred_test_cnn_y = model.predict([test_X], batch_size=1024, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "21260342b5b41fa161b664bcfed0da749802446e",
        "id": "EbuscsBE2gJu"
      },
      "source": [
        "# del word_index, embedding_matrix, model, inp, x\n",
        "import gc; gc.collect()\n",
        "time.sleep(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "17e67a2a48ca69332c6e4edf2493f4c066c625ce",
        "id": "1Vhg8RFF2gJu"
      },
      "source": [
        "pred_val_y = 0.6 * pred_val_lstm_y + 0.4 * pred_val_cnn_y  # two random numbers :)\n",
        "pred_test_y = 0.6 * pred_test_lstm_y + 0.4 * pred_test_cnn_y \n",
        "\n",
        "# f1 score이 최대로 나오는 target 1에 대한 threshold 결정\n",
        "thresholds = []\n",
        "for thresh in np.arange(0.1, 0.501, 0.01):\n",
        "    thresh = np.round(thresh, 2)\n",
        "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
        "    thresholds.append([thresh, res])\n",
        "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
        "    \n",
        "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
        "best_thresh = thresholds[0][0]\n",
        "print(\"Best threshold: \", best_thresh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "26e07c47be2330edb90ae6dd6b87711a71ac42e5",
        "id": "CD8RkSum2gJv"
      },
      "source": [
        "pred_test_y = (pred_test_y > best_thresh).astype(int)\n",
        "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
        "out_df['prediction'] = pred_test_y\n",
        "out_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "f6797ab73bdd5bdb8c8f6d80ec361c50a2b0f56f",
        "id": "PYRqgtQs2gJv"
      },
      "source": [
        "**References:**\n",
        "\n",
        "1. https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout"
      ]
    }
  ]
}