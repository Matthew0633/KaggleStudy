{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D5_lgbm1_lightgbm_with_simple_features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD7pGLxENBkJaMdDdDu6iP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cYjDhwTIA0D"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "- 유용한 피쳐 생성(통계값 이용) : 상환과 납부 간 시간 또는 금액차, 회수금액 비율, 거절 또는 승인 데이터 내의 값들 따로 추가\r\n",
        "- stratifed kfold 시행\r\n",
        "- kfold 별 test set 예측값 평균을 최종제출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVcdwsQuD-vv"
      },
      "source": [
        "# HOME CREDIT DEFAULT RISK COMPETITION\r\n",
        "# Most features are created by applying min, max, mean, sum and var functions to grouped tables. \r\n",
        "# Little feature selection is done and overfitting might be a problem since many features are related.\r\n",
        "# The following key ideas were used:\r\n",
        "# - Divide or subtract important features to get rates (like annuity and income)\r\n",
        "# - In Bureau Data: create specific features for Active credits and Closed credits\r\n",
        "# - In Previous Applications: create specific features for Approved and Refused applications\r\n",
        "# - Modularity: one function for each table (except bureau_balance and application_test)\r\n",
        "# - One-hot encoding for categorical features\r\n",
        "# All tables are joined with the application DF using the SK_ID_CURR key (except bureau_balance).\r\n",
        "# You can use LightGBM with KFold or Stratified KFold.\r\n",
        "\r\n",
        "# Update 16/06/2018:\r\n",
        "# - Added Payment Rate feature\r\n",
        "# - Removed index from features\r\n",
        "# - Use standard KFold CV (not stratified)\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import gc\r\n",
        "import time\r\n",
        "from contextlib import contextmanager\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\r\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import warnings\r\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
        "\r\n",
        "@contextmanager\r\n",
        "def timer(title):\r\n",
        "    t0 = time.time()\r\n",
        "    yield\r\n",
        "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\r\n",
        "\r\n",
        "# One-hot encoding for categorical columns with get_dummies\r\n",
        "def one_hot_encoder(df, nan_as_category = True):\r\n",
        "    original_columns = list(df.columns)\r\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\r\n",
        "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\r\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\r\n",
        "    return df, new_columns\r\n",
        "\r\n",
        "# Preprocess application_train.csv and application_test.csv\r\n",
        "def application_train_test(num_rows = None, nan_as_category = False):\r\n",
        "    # Read data and merge\r\n",
        "    df = pd.read_csv('/content/application_train.csv', nrows= num_rows)\r\n",
        "    test_df = pd.read_csv('/content/application_test.csv', nrows= num_rows)\r\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\r\n",
        "    df = df.append(test_df).reset_index()\r\n",
        "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\r\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\r\n",
        "    \r\n",
        "    # Categorical features with Binary encode (0 or 1; two categories)\r\n",
        "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\r\n",
        "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\r\n",
        "    # Categorical features with One-Hot encode\r\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category)\r\n",
        "    \r\n",
        "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\r\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\r\n",
        "    # Some simple new features (percentages)\r\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\r\n",
        "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\r\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\r\n",
        "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\r\n",
        "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\r\n",
        "    del test_df\r\n",
        "    gc.collect()\r\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBm55rFrV8_W"
      },
      "source": [
        "# Preprocess bureau.csv and bureau_balance.csv\r\n",
        "def bureau_and_balance(num_rows = None, nan_as_category = True):\r\n",
        "    bureau = pd.read_csv('/content/bureau.csv', nrows = num_rows)\r\n",
        "    bb = pd.read_csv('/content/bureau_balance.csv', nrows = num_rows)\r\n",
        "\r\n",
        "    # cat 컬럼 ohe 변환\r\n",
        "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\r\n",
        "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\r\n",
        "    \r\n",
        "    # Bureau balance: Perform aggregations and merge with bureau.csv\r\n",
        "\r\n",
        "    # MONTHS_BALANCE에 대한 count, min, max\r\n",
        "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\r\n",
        "    for col in bb_cat:\r\n",
        "        bb_aggregations[col] = ['mean']\r\n",
        "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\r\n",
        "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\r\n",
        "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\r\n",
        "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\r\n",
        "    del bb, bb_agg\r\n",
        "    gc.collect()\r\n",
        "    \r\n",
        "\r\n",
        "    # num_features min, max, mean, var 등 통계값 계산, cat은 mean 사용\r\n",
        "\r\n",
        "    # Bureau and bureau_balance numeric features\r\n",
        "    num_aggregations = {\r\n",
        "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\r\n",
        "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\r\n",
        "        'DAYS_CREDIT_UPDATE': ['mean'],\r\n",
        "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\r\n",
        "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\r\n",
        "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\r\n",
        "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\r\n",
        "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\r\n",
        "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\r\n",
        "        'AMT_ANNUITY': ['max', 'mean'],\r\n",
        "        'CNT_CREDIT_PROLONG': ['sum'],\r\n",
        "        'MONTHS_BALANCE_MIN': ['min'],\r\n",
        "        'MONTHS_BALANCE_MAX': ['max'],\r\n",
        "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\r\n",
        "    }\r\n",
        "    # Bureau and bureau_balance categorical features\r\n",
        "    cat_aggregations = {}\r\n",
        "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\r\n",
        "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\r\n",
        "    \r\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\r\n",
        "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\r\n",
        "\r\n",
        "    # credit active 데이터들에 대해 num features들에 대한 통계값 따로 계산\r\n",
        "    # Bureau: Active credits - using only numerical aggregations\r\n",
        "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\r\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\r\n",
        "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\r\n",
        "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\r\n",
        "    del active, active_agg\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    # credit closed 데이터들에 대해 num features들에 대한 통계값 따로 계산\r\n",
        "    # Bureau: Closed credits - using only numerical aggregations\r\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\r\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\r\n",
        "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\r\n",
        "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\r\n",
        "    del closed, closed_agg, bureau\r\n",
        "    gc.collect()\r\n",
        "    return bureau_agg\r\n",
        "\r\n",
        "# Preprocess previous_applications.csv\r\n",
        "def previous_applications(num_rows = None, nan_as_category = True):\r\n",
        "    prev = pd.read_csv('/content/previous_application.csv', nrows = num_rows)\r\n",
        "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\r\n",
        "\r\n",
        "    # Days 365.243 values -> nan (잘못된 값을 결측치로 대체)\r\n",
        "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\r\n",
        "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\r\n",
        "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\r\n",
        "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\r\n",
        "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\r\n",
        "    \r\n",
        "    # 회수금액 비율 변수 추가생성\r\n",
        "    # Add feature: value ask / value received percentage\r\n",
        "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\r\n",
        "\r\n",
        "    # num feature에 대해서 통계값 계산\r\n",
        "    # Previous applications numeric features\r\n",
        "    num_aggregations = {\r\n",
        "        'AMT_ANNUITY': ['min', 'max', 'mean'],\r\n",
        "        'AMT_APPLICATION': ['min', 'max', 'mean'],\r\n",
        "        'AMT_CREDIT': ['min', 'max', 'mean'],\r\n",
        "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\r\n",
        "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\r\n",
        "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\r\n",
        "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\r\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\r\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],\r\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\r\n",
        "    }\r\n",
        "    # Previous applications categorical features\r\n",
        "    cat_aggregations = {}\r\n",
        "    for cat in cat_cols:\r\n",
        "        cat_aggregations[cat] = ['mean']\r\n",
        "    \r\n",
        "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\r\n",
        "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\r\n",
        "\r\n",
        "    # 승인된 대출신청 data 통계값\r\n",
        "    # Previous Applications: Approved Applications - only numerical features\r\n",
        "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\r\n",
        "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\r\n",
        "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\r\n",
        "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\r\n",
        "\r\n",
        "    # 거절된 대출신청 data 통계값\r\n",
        "    # Previous Applications: Refused Applications - only numerical features\r\n",
        "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\r\n",
        "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\r\n",
        "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\r\n",
        "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\r\n",
        "    del refused, refused_agg, approved, approved_agg, prev\r\n",
        "    gc.collect()\r\n",
        "    return prev_agg\r\n",
        "\r\n",
        "# Preprocess POS_CASH_balance.csv\r\n",
        "def pos_cash(num_rows = None, nan_as_category = True):\r\n",
        "    pos = pd.read_csv('/content/POS_CASH_balance.csv', nrows = num_rows)\r\n",
        "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\r\n",
        "    # Features\r\n",
        "    aggregations = {\r\n",
        "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\r\n",
        "        'SK_DPD': ['max', 'mean'],\r\n",
        "        'SK_DPD_DEF': ['max', 'mean']\r\n",
        "    }\r\n",
        "    for cat in cat_cols:\r\n",
        "        aggregations[cat] = ['mean']\r\n",
        "    \r\n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\r\n",
        "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\r\n",
        "\r\n",
        "    # account 개수\r\n",
        "    # Count pos cash accounts\r\n",
        "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\r\n",
        "    del pos\r\n",
        "    gc.collect()\r\n",
        "    return pos_agg\r\n",
        "    \r\n",
        "# Preprocess installments_payments.csv\r\n",
        "def installments_payments(num_rows = None, nan_as_category = True):\r\n",
        "    ins = pd.read_csv('/content/installments_payments.csv', nrows = num_rows)\r\n",
        "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\r\n",
        "\r\n",
        "    # 상환액/할부금 비율 또는 차이\r\n",
        "    # Percentage and difference paid in each installment (amount paid and installment value)\r\n",
        "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\r\n",
        "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\r\n",
        "\r\n",
        "    # 납부일과 상환일 차이\r\n",
        "    # Days past due and days before due (no negative values)\r\n",
        "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\r\n",
        "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\r\n",
        "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\r\n",
        "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\r\n",
        "\r\n",
        "    # Features: Perform aggregations\r\n",
        "    aggregations = {\r\n",
        "        'NUM_INSTALMENT_VERSION': ['nunique'],\r\n",
        "        'DPD': ['max', 'mean', 'sum'],\r\n",
        "        'DBD': ['max', 'mean', 'sum'],\r\n",
        "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\r\n",
        "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\r\n",
        "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\r\n",
        "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\r\n",
        "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\r\n",
        "    }\r\n",
        "    for cat in cat_cols:\r\n",
        "        aggregations[cat] = ['mean']\r\n",
        "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\r\n",
        "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\r\n",
        "\r\n",
        "    # install 데이터 개수 추가\r\n",
        "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\r\n",
        "    del ins\r\n",
        "    gc.collect()\r\n",
        "    return ins_agg\r\n",
        "\r\n",
        "# Preprocess credit_card_balance.csv\r\n",
        "def credit_card_balance(num_rows = None, nan_as_category = True):\r\n",
        "    cc = pd.read_csv('/content/credit_card_balance.csv', nrows = num_rows)\r\n",
        "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\r\n",
        "\r\n",
        "    # General aggregations\r\n",
        "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\r\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\r\n",
        "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\r\n",
        "    \r\n",
        "    # Count credit card lines\r\n",
        "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\r\n",
        "    del cc\r\n",
        "    gc.collect()\r\n",
        "    return cc_agg\r\n",
        "\r\n",
        "# LightGBM GBDT with KFold or Stratified KFold\r\n",
        "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\r\n",
        "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\r\n",
        "    # Divide in training/validation and test data\r\n",
        "    train_df = df[df['TARGET'].notnull()]\r\n",
        "    test_df = df[df['TARGET'].isnull()]\r\n",
        "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\r\n",
        "    del df\r\n",
        "    gc.collect()\r\n",
        "    \r\n",
        "    # stratifed kfold 수행\r\n",
        "    if stratified:\r\n",
        "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\r\n",
        "    else:\r\n",
        "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\r\n",
        "\r\n",
        "    # Create arrays and dataframes to store results\r\n",
        "    oof_preds = np.zeros(train_df.shape[0])\r\n",
        "    sub_preds = np.zeros(test_df.shape[0])\r\n",
        "    feature_importance_df = pd.DataFrame()\r\n",
        "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\r\n",
        "    \r\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\r\n",
        "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\r\n",
        "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\r\n",
        "\r\n",
        "        # LightGBM parameters found by Bayesian optimization\r\n",
        "        clf = LGBMClassifier(\r\n",
        "            nthread=4,\r\n",
        "            n_estimators=10000,\r\n",
        "            learning_rate=0.02,\r\n",
        "            num_leaves=34,\r\n",
        "            colsample_bytree=0.9497036,\r\n",
        "            subsample=0.8715623,\r\n",
        "            max_depth=8,\r\n",
        "            reg_alpha=0.041545473,\r\n",
        "            reg_lambda=0.0735294,\r\n",
        "            min_split_gain=0.0222415,\r\n",
        "            min_child_weight=39.3259775,\r\n",
        "            silent=-1,\r\n",
        "            verbose=-1, )\r\n",
        "\r\n",
        "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \r\n",
        "            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\r\n",
        "\r\n",
        "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\r\n",
        "\r\n",
        "        # fold별 학습한 모델이 test set을 예측한 평균값을 submit\r\n",
        "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\r\n",
        "\r\n",
        "        fold_importance_df = pd.DataFrame()\r\n",
        "        fold_importance_df[\"feature\"] = feats\r\n",
        "        fold_importance_df[\"importance\"] = clf.feature_importances_\r\n",
        "        fold_importance_df[\"fold\"] = n_fold + 1\r\n",
        "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\r\n",
        "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\r\n",
        "        del clf, train_x, train_y, valid_x, valid_y\r\n",
        "        gc.collect()\r\n",
        "\r\n",
        "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\r\n",
        "    # Write submission file and plot feature importance\r\n",
        "    if not debug:\r\n",
        "        test_df['TARGET'] = sub_preds\r\n",
        "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\r\n",
        "    display_importances(feature_importance_df)\r\n",
        "    return feature_importance_df\r\n",
        "\r\n",
        "# 피쳐 중요도 시각화\r\n",
        "def display_importances(feature_importance_df_):\r\n",
        "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\r\n",
        "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\r\n",
        "    plt.figure(figsize=(8, 10))\r\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\r\n",
        "    plt.title('LightGBM Features (avg over folds)')\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('lgbm_importances01.png')\r\n",
        "\r\n",
        "\r\n",
        "def main(debug = False):\r\n",
        "    num_rows = 10000 if debug else None\r\n",
        "    df = application_train_test(num_rows)\r\n",
        "    with timer(\"Process bureau and bureau_balance\"):\r\n",
        "        bureau = bureau_and_balance(num_rows)\r\n",
        "        print(\"Bureau df shape:\", bureau.shape)\r\n",
        "        df = df.join(bureau, how='left', on='SK_ID_CURR')\r\n",
        "        del bureau\r\n",
        "        gc.collect()\r\n",
        "    with timer(\"Process previous_applications\"):\r\n",
        "        prev = previous_applications(num_rows)\r\n",
        "        print(\"Previous applications df shape:\", prev.shape)\r\n",
        "        df = df.join(prev, how='left', on='SK_ID_CURR')\r\n",
        "        del prev\r\n",
        "        gc.collect()\r\n",
        "    with timer(\"Process POS-CASH balance\"):\r\n",
        "        pos = pos_cash(num_rows)\r\n",
        "        print(\"Pos-cash balance df shape:\", pos.shape)\r\n",
        "        df = df.join(pos, how='left', on='SK_ID_CURR')\r\n",
        "        del pos\r\n",
        "        gc.collect()\r\n",
        "    with timer(\"Process installments payments\"):\r\n",
        "        ins = installments_payments(num_rows)\r\n",
        "        print(\"Installments payments df shape:\", ins.shape)\r\n",
        "        df = df.join(ins, how='left', on='SK_ID_CURR')\r\n",
        "        del ins\r\n",
        "        gc.collect()\r\n",
        "    with timer(\"Process credit card balance\"):\r\n",
        "        cc = credit_card_balance(num_rows)\r\n",
        "        print(\"Credit card balance df shape:\", cc.shape)\r\n",
        "        df = df.join(cc, how='left', on='SK_ID_CURR')\r\n",
        "        del cc\r\n",
        "        gc.collect()\r\n",
        "    with timer(\"Run LightGBM with kfold\"):\r\n",
        "        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    submission_file_name = \"submission_kernel02.csv\"\r\n",
        "    with timer(\"Full model run\"):\r\n",
        "        main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}