{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "Ridge9_avito-lightgbm-with-ridge-feature-v-3-0-0-2219.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeA3gpGK8U7-"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "- 추가변수 생성 - number of Alphabets, number of AlphaNumeric, number of Digits, title_desc_len_ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8jRX3le8W14"
      },
      "source": [
        "# 2.0에 비해 변수추가 - number of Alphabets, number of AlphaNumeric, number of Digits, title_desc_len_ratio\r\n",
        "\r\n",
        "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\r\n",
        "#improvement using kernel from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm\r\n",
        "#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\r\n",
        "#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\r\n",
        "#Forked From - https://www.kaggle.com/him4318/avito-lightgbm-with-ridge-feature-v-2-0\r\n",
        "\r\n",
        "import time\r\n",
        "notebookstart= time.time()\r\n",
        "\r\n",
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import os\r\n",
        "import gc\r\n",
        "import random\r\n",
        "random.seed(2018)\r\n",
        "print(\"Data:\\n\",os.listdir(\"../input\"))\r\n",
        "\r\n",
        "# Models Packages\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn import feature_selection\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn import preprocessing\r\n",
        "\r\n",
        "# Gradient Boosting\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.linear_model import Ridge\r\n",
        "from sklearn.cross_validation import KFold\r\n",
        "\r\n",
        "# Tf-Idf\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.pipeline import FeatureUnion\r\n",
        "from scipy.sparse import hstack, csr_matrix\r\n",
        "from nltk.corpus import stopwords \r\n",
        "\r\n",
        "# Viz\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import string\r\n",
        "\r\n",
        "NFOLDS = 5\r\n",
        "SEED = 2018\r\n",
        "VALID = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyI21vfx8aFf"
      },
      "source": [
        "class SklearnWrapper(object):\r\n",
        "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\r\n",
        "        if(seed_bool == True):\r\n",
        "            params['random_state'] = seed\r\n",
        "        self.clf = clf(**params)\r\n",
        "\r\n",
        "    def train(self, x_train, y_train):\r\n",
        "        self.clf.fit(x_train, y_train)\r\n",
        "\r\n",
        "    def predict(self, x):\r\n",
        "        return self.clf.predict(x)\r\n",
        "        \r\n",
        "def get_oof(clf, x_train, y, x_test):\r\n",
        "    oof_train = np.zeros((ntrain,))\r\n",
        "    oof_test = np.zeros((ntest,))\r\n",
        "    oof_test_skf = np.empty((NFOLDS, ntest))\r\n",
        "\r\n",
        "    for i, (train_index, test_index) in enumerate(kf):\r\n",
        "        print('\\nFold {}'.format(i))\r\n",
        "        x_tr = x_train[train_index]\r\n",
        "        y_tr = y[train_index]\r\n",
        "        x_te = x_train[test_index]\r\n",
        "\r\n",
        "        clf.train(x_tr, y_tr)\r\n",
        "\r\n",
        "        oof_train[test_index] = clf.predict(x_te)\r\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\r\n",
        "\r\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\r\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\r\n",
        "    \r\n",
        "def cleanName(text):\r\n",
        "    try:\r\n",
        "        textProc = text.lower()\r\n",
        "        # textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\r\n",
        "        #regex = re.compile(u'[^[:alpha:]]')\r\n",
        "        #textProc = regex.sub(\" \", textProc)\r\n",
        "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\r\n",
        "        textProc = \" \".join(textProc.split())\r\n",
        "        return textProc\r\n",
        "    except: \r\n",
        "        return \"name error\"\r\n",
        "    \r\n",
        "    \r\n",
        "def rmse(y, y0):\r\n",
        "    assert len(y) == len(y0)\r\n",
        "    return np.sqrt(np.mean(np.power((y - y0), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "trusted": false,
        "id": "Ui-xqG2a6B1K"
      },
      "source": [
        "print(\"\\nData Load Stage\")\n",
        "training = pd.read_csv('../input/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
        "traindex = training.index\n",
        "testing = pd.read_csv('../input/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
        "testdex = testing.index\n",
        "\n",
        "ntrain = training.shape[0]\n",
        "ntest = testing.shape[0]\n",
        "\n",
        "kf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "y = training.deal_probability.copy()\n",
        "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
        "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
        "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n",
        "\n",
        "print(\"Combine Train and Test\")\n",
        "df = pd.concat([training,testing],axis=0)\n",
        "del training, testing\n",
        "gc.collect()\n",
        "print('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n",
        "\n",
        "\n",
        "print(\"Feature Engineering\")\n",
        "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
        "df[\"price\"].fillna(df.price.mean(),inplace=True)\n",
        "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
        "\n",
        "print(\"\\nCreate Time Variables\")\n",
        "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
        "#df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
        "#df[\"Day of Month\"] = df['activation_date'].dt.day\n",
        "\n",
        "# Create Validation Index and Remove Dead Variables\n",
        "training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
        "validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
        "df.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n",
        "\n",
        "print(\"\\nEncode Variables\")\n",
        "categorical = [\"user_id\",\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\"]\n",
        "print(\"Encoding :\",categorical)\n",
        "\n",
        "# Encoder:\n",
        "lbl = preprocessing.LabelEncoder()\n",
        "for col in categorical:\n",
        "    df[col].fillna('Unknown')\n",
        "    df[col] = lbl.fit_transform(df[col].astype(str))\n",
        "    \n",
        "print(\"\\nText Features\")\n",
        "\n",
        "# Feature Engineering \n",
        "\n",
        "# Meta Text Features\n",
        "textfeats = [\"description\", \"title\"]\n",
        "df['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "df['title'] = df['title'].apply(lambda x: cleanName(x))\n",
        "df[\"description\"]   = df[\"description\"].apply(lambda x: cleanName(x))\n",
        "\n",
        "\n",
        "# 추가변수 생성 - number of Alphabets, number of AlphaNumeric, number of Digits, title_desc_len_ratio\n",
        "for cols in textfeats:\n",
        "    df[cols] = df[cols].astype(str) \n",
        "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
        "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
        "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
        "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
        "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
        "    df[cols + '_num_letters'] = df[cols].apply(lambda comment: len(comment)) # Count number of Letters\n",
        "    df[cols + '_num_alphabets'] = df[cols].apply(lambda comment: (comment.count(r'[a-zA-Z]'))) # Count number of Alphabets\n",
        "    df[cols + '_num_alphanumeric'] = df[cols].apply(lambda comment: (comment.count(r'[A-Za-z0-9]'))) # Count number of AlphaNumeric\n",
        "    df[cols + '_num_digits'] = df[cols].apply(lambda comment: (comment.count('[0-9]'))) # Count number of Digits\n",
        "    \n",
        "# Extra Feature Engineering\n",
        "df['title_desc_len_ratio'] = df['title_num_letters']/df['description_num_letters']\n",
        "\n",
        "print(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\n",
        "russian_stop = set(stopwords.words('russian'))\n",
        "\n",
        "tfidf_para = {\n",
        "    \"stop_words\": russian_stop,\n",
        "    \"analyzer\": 'word',\n",
        "    \"token_pattern\": r'\\w{1,}',\n",
        "    \"sublinear_tf\": True,\n",
        "    \"dtype\": np.float32,\n",
        "    \"norm\": 'l2',\n",
        "    #\"min_df\":5,\n",
        "    #\"max_df\":.9,\n",
        "    \"smooth_idf\":False\n",
        "}\n",
        "\n",
        "\n",
        "def get_col(col_name): return lambda x: x[col_name]\n",
        "##I added to the max_features of the description. It did not change my score much but it may be worth investigating\n",
        "vectorizer = FeatureUnion([\n",
        "        ('description',TfidfVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=17000,\n",
        "            **tfidf_para,\n",
        "            preprocessor=get_col('description'))),\n",
        "        ('title',CountVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            stop_words = russian_stop,\n",
        "            #max_features=7000,\n",
        "            preprocessor=get_col('title')))\n",
        "    ])\n",
        "    \n",
        "start_vect=time.time()\n",
        "\n",
        "#Fit my vectorizer on the entire dataset instead of the training rows\n",
        "#Score improved by .0001\n",
        "vectorizer.fit(df.to_dict('records'))\n",
        "\n",
        "ready_df = vectorizer.transform(df.to_dict('records'))\n",
        "tfvocab = vectorizer.get_feature_names()\n",
        "print(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
        "\n",
        "# Drop Text Cols\n",
        "textfeats = [\"description\", \"title\"]\n",
        "df.drop(textfeats, axis=1,inplace=True)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "ridge_params = {'alpha':30.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
        "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n",
        "\n",
        "#Ridge oof method from Faron's kernel\n",
        "#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n",
        "#It doesn't really add much to the score, but it does help lightgbm converge faster\n",
        "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
        "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])\n",
        "\n",
        "rms = sqrt(mean_squared_error(y, ridge_oof_train))\n",
        "print('Ridge OOF RMSE: {}'.format(rms))\n",
        "\n",
        "print(\"Modeling Stage\")\n",
        "\n",
        "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
        "\n",
        "df['ridge_preds'] = ridge_preds\n",
        "\n",
        "# Combine Dense Features with Sparse Text Bag of Words Features\n",
        "X = hstack([csr_matrix(df.loc[traindex,:].values),ready_df[0:traindex.shape[0]]]) # Sparse Matrix\n",
        "testing = hstack([csr_matrix(df.loc[testdex,:].values),ready_df[traindex.shape[0]:]])\n",
        "tfvocab = df.columns.tolist() + tfvocab\n",
        "for shape in [X,testing]:\n",
        "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
        "print(\"Feature Names Length: \",len(tfvocab))\n",
        "del df\n",
        "gc.collect();\n",
        "\n",
        "print(\"\\nModeling Stage\")\n",
        "\n",
        "del ridge_preds,vectorizer,ready_df\n",
        "gc.collect();\n",
        "    \n",
        "print(\"Light Gradient Boosting Regressor\")\n",
        "lgbm_params =  {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    # 'max_depth': 15,\n",
        "    'num_leaves': 270,\n",
        "    'feature_fraction': 0.5,\n",
        "    'bagging_fraction': 0.75,\n",
        "    'bagging_freq': 2,\n",
        "    'learning_rate': 0.0175,\n",
        "    'verbose': 0\n",
        "}  \n",
        "\n",
        "\n",
        "if VALID == True:\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "        X, y, test_size=0.10, random_state=2018)\n",
        "        \n",
        "    # LGBM Dataset Formatting \n",
        "    lgtrain = lgb.Dataset(X_train, y_train,\n",
        "                    feature_name=tfvocab,\n",
        "                    categorical_feature = categorical)\n",
        "    lgvalid = lgb.Dataset(X_valid, y_valid,\n",
        "                    feature_name=tfvocab,\n",
        "                    categorical_feature = categorical)\n",
        "    del X, X_train; gc.collect()\n",
        "    \n",
        "    # Go Go Go\n",
        "    lgb_clf = lgb.train(\n",
        "        lgbm_params,\n",
        "        lgtrain,\n",
        "        num_boost_round=20000,\n",
        "        valid_sets=[lgtrain, lgvalid],\n",
        "        valid_names=['train','valid'],\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "    print(\"Model Evaluation Stage\")\n",
        "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
        "    del X_valid ; gc.collect()\n",
        "\n",
        "else:\n",
        "    # LGBM Dataset Formatting \n",
        "    lgtrain = lgb.Dataset(X, y,\n",
        "                    feature_name=tfvocab,\n",
        "                    categorical_feature = categorical)\n",
        "    del X; gc.collect()\n",
        "    # Go Go Go\n",
        "    lgb_clf = lgb.train(\n",
        "        lgbm_params,\n",
        "        lgtrain,\n",
        "        num_boost_round=2250,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Feature Importance Plot\n",
        "f, ax = plt.subplots(figsize=[7,10])\n",
        "lgb.plot_importance(lgb_clf, max_num_features=100, ax=ax)\n",
        "plt.title(\"Light GBM Feature Importance\")\n",
        "plt.savefig('feature_import.png')\n",
        "\n",
        "print(\"Model Evaluation Stage\")\n",
        "lgpred = lgb_clf.predict(testing) \n",
        "\n",
        "#Mixing lightgbm with ridge. I haven't really tested if this improves the score or not\n",
        "#blend = 0.95*lgpred + 0.05*ridge_oof_test[:,0]\n",
        "lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
        "lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
        "lgsub.to_csv(\"lgsub.csv\",index=True,header=True)\n",
        "#print(\"Model Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))\n",
        "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}