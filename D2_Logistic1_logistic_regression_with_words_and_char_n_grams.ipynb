{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "logistic-regression-with-words-and-char-n-grams.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2rvKHWxA82"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "- unigram 1 word + tfidf 로 text feature화\r\n",
        "- Logistic + CV 수행\r\n",
        "- Logistic으로 multi-classification 수행시에는 predict_proba()[1]로 각 class의 확률을 구하여 결정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2298e25d-0bf0-446d-a9e2-16f0ac09bc1d",
        "_cell_guid": "c0c1c95c-6c37-4b4f-9b6d-8303edc6c465",
        "trusted": true,
        "id": "xAbfxdo0w0a_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "train = pd.read_csv('../input/train.csv.zip').fillna(' ')\n",
        "test = pd.read_csv('../input/test.csv.zip').fillna(' ')\n",
        "\n",
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']\n",
        "all_text = pd.concat([train_text, test_text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NXuvAI4Pw0bH"
      },
      "source": [
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=10000)\n",
        "word_vectorizer.fit(all_text)\n",
        "train_word_features = word_vectorizer.transform(train_text)\n",
        "test_word_features = word_vectorizer.transform(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cuCdvM6Yw0bI"
      },
      "source": [
        "train_word_features.shape, test_word_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9UKuums-w0bI"
      },
      "source": [
        "# 1 unigram (1 word)\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse =True)[:20])\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse =False)[1000])\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse =False)[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mBVOBrwRw0bI"
      },
      "source": [
        "char_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    stop_words='english',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=50000)\n",
        "\n",
        "char_vectorizer.fit(all_text)\n",
        "train_char_features = char_vectorizer.transform(train_text)\n",
        "test_char_features = char_vectorizer.transform(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o52OLVKPw0bJ"
      },
      "source": [
        "# 1 unigram (1 word)\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse = True)[:20])\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse = False)[1000])\n",
        "print(sorted(word_vectorizer.vocabulary_.items(),reverse = False)[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_hY8aqGMw0bJ"
      },
      "source": [
        "train_features = hstack([train_char_features, train_word_features])\n",
        "test_features = hstack([test_char_features, test_word_features])\n",
        "scores = []\n",
        "submission = pd.DataFrame.from_dict({'id': test['id']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U2pgssKdw0bJ"
      },
      "source": [
        "for class_name in class_names:\n",
        "    train_target = train[class_name]\n",
        "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
        "\n",
        "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
        "    scores.append(cv_score)\n",
        "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
        "\n",
        "    classifier.fit(train_features, train_target)\n",
        "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('Total CV score is {}'.format(np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n33e9zQDw0bK"
      },
      "source": [
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A6JDtF4yw0bK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yGGNS9MMw0bK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}