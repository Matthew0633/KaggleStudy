{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "D6_DL10_rectified-linear-units-relu-in-deep-learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8MXpgD-zbU6"
      },
      "source": [
        "## Impressions and Reviews  \r\n",
        "- relu 사용 이유 : non linearity, interactive effect \r\n",
        "- interactive effect (다른 파라미터에 의해 1, 0으로 결과값 상이)\r\n",
        "- non linearity (bias, multiple node)\r\n",
        "- tanh (-2,2 이외에 vanishing gradient 문제 발생 up, relu에서는 0, 에서는 해결. 0이하에서도 보완하기위한 leaky relu 존재)\r\n",
        "- 실제로 relu가 가장 잘 작동"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bffc40c2-79f9-48c4-952f-cc63e9280ef5",
        "_uuid": "69b82ea86d79947ed9aedf26abd8d39230186f02",
        "id": "du93OhR2KnST"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The Rectified Linear Unit is the most commonly used activation function in deep learning models.  The function returns 0 if it receives any negative input, but for any positive value $x$ it returns that value back. So it can be written as  $f(x) = max(0,x)$.  \n",
        "\n",
        "Graphically it looks like this\n",
        "\n",
        "![ReLU image](https://i.imgur.com/gKA4kA9.jpg)\n",
        "\n",
        "It's surprising that such a simple function (and one composed of two linear pieces) can allow your model to account for non-linearities and interactions so well.  But the ReLU function works great in most applications, and it is very widely used as a result.\n",
        "\n",
        "# Why It Works\n",
        "## Introducing Interactions and Non-linearities\n",
        "\n",
        "Activation functions serve two primary purposes:\n",
        "1) Help a model account for **interaction effects**.  \n",
        "What is an interactive effect?  It is when one variable A affects a prediction differently depending on the value of B. For example, if my model wanted to know whether a certain body weight indicated an increased risk of diabetes, it would have to know an individual's height.  Some bodyweights indicate elevated risks for short people, while indicating good health for tall people.  So, the **effect of body weight on diabetes risk depends on height**, and we would say that **weight and height have an interaction effect**.\n",
        "\n",
        "2) Help a model account for **non-linear effects.**\n",
        "This just means that if I graph a variable on the horizontal axis, and my predictions on the vertical axis, it isn't a straight line.  Or said another way, the effect of increasing the predictor by one is different at different values of that predictor.\n",
        "\n",
        "상호 작용 및 비선형성 소개\n",
        "활성화 기능은 두 가지 주요 목적으로 사용됩니다. 1) 모델이 상호 작용 효과를 설명하도록 도와줍니다.\n",
        "인터랙티브 효과 란 무엇입니까? 하나의 변수 A가 B의 값에 따라 예측에 다르게 영향을 미치는 경우입니다. 예를 들어, 내 모델이 특정 체중이 당뇨병 위험 증가를 나타내는 지 여부를 알고 싶다면 개인의 키를 알아야합니다. 일부 체중은 키가 작은 사람에게는 높은 위험을 나타내는 반면 키가 큰 사람에게는 좋은 건강을 나타냅니다. 따라서 체중이 당뇨병 위험에 미치는 영향은 신장에 따라 달라지며 체중과 신장은 상호 작용 효과가 있다고 말할 수 있습니다.\n",
        "\n",
        "2) 모델이 비선형 효과를 설명하도록 도와줍니다. 이것은 가로축에 변수를 그래프로 표시하고 세로축에 내 예측을 그래프로 표시하면 직선이 아니라는 것을 의미합니다. 또는 다른 방식으로 말하면, 예측 변수를 하나씩 증가시키는 효과는 해당 예측 변수의 다른 값에서 다릅니다.\n",
        "\n",
        "## How ReLU captures Interactions and Non-Linearities\n",
        "** Interactions:** Imagine a single node in a neural network model.  For simplicity, assume it has two inputs, called A and B.  The weights from A and B into our node are 2 and 3 respectively.  So the node output is $f(2A + 3B)$. We'll use the ReLU function for our f.  So, if $2A + 3B$ is positive, the output value of our node is also $2A + 3B$. If $2A + 3B$ is negative, the output value of our node is 0.\n",
        "\n",
        "For concreteness, consider a case where A=1 and B=1.  The output is $2A + 3B$, and if A increases, then the output increases too.  On the other hand, if B=-100 then the output is 0, and if A increases moderately, the output remains 0.  So A might increase our output, or it might not.  It just depends what the value of B is.\n",
        "\n",
        "This is a simple case where the node captured an interaction. As you add more nodes and more layers, the potential complexity of interactions only increases.  But you should now see how the activation function helped capture an interaction.\n",
        "\n",
        "**Non-linearities:**  A function is non-linear if the slope isn't constant.  So, the ReLU function is non-linear around 0, but the slope is always either 0 (for negative values) or 1 (for positive values).  That's a very limited type of non-linearity.  \n",
        "\n",
        "But two facts about deep learning models allow us to create many different types of non-linearities from how we combine ReLU nodes.\n",
        "\n",
        "First, most models include a **bias** term for each node.  The bias term is just a constant number that is determined during model training.  For simplicity, consider a node with a single input called A, and a bias.  If the bias term takes a value of 7, then the node output is f(7+A).  In this case, if A is less than -7, the output is 0 and the slope is 0.  If A is greater than -7, then the node's output is 7+A, and the slope is 1.  \n",
        "\n",
        "So the bias term allows us to move where the slope changes. So far, it still appears we can have only two different slopes.\n",
        "\n",
        "However, real models have many nodes. Each node (even within a single layer) can have a different value for it's bias, so each node can change slope at different values for our input.  \n",
        "\n",
        "When we add the resulting functions back up, we get a combined function that changes slopes in many places.\n",
        "\n",
        "These models have the flexibility to produce non-linear functions and account for interactions well (if that will giv better predictions).  As we add more nodes in each layer (or more convolutions if we are using a convolutional model) the model gets even greater ability to represent these interactions and non-linearities.\n",
        "\n",
        "** 상호 작용 : ** 신경망 모델에서 단일 노드를 상상해보십시오. 간단히하기 위해 A와 B라는 두 개의 입력이 있다고 가정합니다. A와 B에서 노드로의 가중치는 각각 2와 3입니다. 따라서 노드 출력은 f (2A + 3B)입니다. f에 ReLU 함수를 사용할 것입니다. 따라서 2A + 3B가 양수이면 노드의 출력 값도 2A + 3B입니다. 2A + 3B가 음수이면 노드의 출력 값은 0입니다.\n",
        "\n",
        "구체적으로 A = 1 및 B = 1 인 경우를 고려하십시오. 출력은 2A + 3B이고 A가 증가하면 출력도 증가합니다. 반면에 B = -100이면 출력은 0이고 A가 적당히 증가하면 출력은 0으로 유지됩니다. 따라서 A는 출력을 증가시킬 수도 있고 그렇지 않을 수도 있습니다. 그것은 B의 가치가 무엇인지에 달려 있습니다.\n",
        "\n",
        "이것은 노드가 상호 작용을 캡처 한 간단한 경우입니다. 더 많은 노드와 더 많은 계층을 추가할수록 상호 작용의 잠재적 복잡성이 증가 할뿐입니다. 그러나 이제 활성화 기능이 상호 작용을 캡처하는 데 어떻게 도움이되었는지 확인해야합니다.\n",
        "\n",
        "비선형 성 : 기울기가 일정하지 않은 경우 함수는 비선형입니다. 따라서 ReLU 함수는 0을 중심으로 비선형이지만 기울기는 항상 0 (음수 값) 또는 1 (양수 값)입니다. 이것은 매우 제한된 유형의 비선형 성입니다.\n",
        "\n",
        "그러나 딥 러닝 모델에 대한 두 가지 사실을 통해 ReLU 노드를 결합하는 방식에서 다양한 유형의 비선형 성을 생성 할 수 있습니다.\n",
        "\n",
        "첫째, 대부분의 모델에는 각 노드에 대한 편향 용어가 포함되어 있습니다. 편향 항은 모델 학습 중에 결정되는 상수 일뿐입니다. 단순화를 위해 A라는 단일 입력과 편향이있는 노드를 고려하십시오. 바이어스 항의 값이 7이면 노드 출력은 f (7 + A)입니다. 이 경우 A가 -7보다 작 으면 출력은 0이고 기울기는 0입니다. A가 -7보다 크면 노드의 출력은 7 + A이고 기울기는 1입니다.\n",
        "\n",
        "따라서 바이어스 항을 사용하면 기울기가 변하는 곳으로 이동할 수 있습니다. 지금까지 우리는 두 개의 다른 슬로프 만 가질 수있는 것으로 보입니다.\n",
        "\n",
        "그러나 실제 모델에는 많은 노드가 있습니다. 각 노드 (단일 레이어 내에서도)는 바이어스에 대해 서로 다른 값을 가질 수 있으므로 각 노드는 입력에 대해 서로 다른 값에서 기울기를 변경할 수 있습니다.\n",
        "\n",
        "결과 함수를 다시 추가하면 여러 곳에서 경사를 변경하는 결합 함수를 얻습니다.\n",
        "\n",
        "이러한 모델은 비선형 함수를 생성하고 상호 작용을 잘 설명 할 수있는 유연성을 가지고 있습니다 (더 나은 예측을 제공 할 경우). 각 레이어에 더 많은 노드를 추가하면 (또는 컨볼 루션 모델을 사용하는 경우 더 많은 컨볼 루션) 모델은 이러한 상호 작용과 비선형 성을 나타내는 능력이 훨씬 더 커집니다.\n",
        "\n",
        "\n",
        "## Facilitating Gradient Descent\n",
        "This section is more technical than those above it. If you find it difficult, remember that you can have a lot of success using deep learning even without this technical background.\n",
        "\n",
        "Historically, deep learning models started off with s-shaped curves (like the tanh function below)\n",
        "![Imgur](https://i.imgur.com/Q1jQejl.png)\n",
        "\n",
        "The tanh would seem to have a couple advantages.  Even though it gets close to flat, it isn't completely flat anywhere.  So it's output always reflects changes in it's input, which we might expect to be a good thing.  Secondly, it is non-linear (or curved everywhere).  Accounting for non-linearities is one of the activation function's main purposes.  So, we expect a non-linear function to work well.\n",
        "\n",
        "However researchers had great difficulty building models with many layers when using the tanh function.  It is relatively flat except for a very narrow range (that range being about -2 to 2).  The derivative of the function is very small unless the input is in this narrow range, and this flat derivative makes it difficult to improve the weights through gradient descent.  This problem gets worse as the model has more layers.  This was called the **vanishing gradient problem**.\n",
        "\n",
        "The ReLU function has a derivative of 0 over half it's range (the negative numbers).  For positive inputs, the derivative is 1.\n",
        "\n",
        "When training on a reasonable sized batch, there will usually be some data points giving positive values to any given node.  So the average derivative is rarely close to 0, which allows gradient descent to keep progressing.\n",
        "\n",
        "# Alternatives\n",
        "\n",
        "There are many similar alternatives which also work well.  The Leaky ReLU is one of the most well known.  It is the same as ReLU for positive numbers.  But instead of being 0 for all negative values, it has a constant slope (less than 1.). \n",
        "\n",
        "That slope is a parameter the user sets when building the model, and it is frequently called $\\alpha$.  For example, if the user sets $\\alpha = 0.3$, the activation function is `f(x) = max(0.3*x, x)`.  This has the theoretical advantage that, by being influenced by `x` at all values, it may be make more complete use of the information contained in `x`.  \n",
        "\n",
        "Their are other alternatives, but both practitioners and researchers have generally found insufficient benefit to justify using anything other than ReLU."
      ]
    }
  ]
}